# üìä Token Counter Module

Module ƒë·∫øm v√† theo d√µi token usage cho t·∫•t c·∫£ LLM providers v·ªõi thi·∫øt k·∫ø d·ªÖ m·ªü r·ªông v√† t√°i s·ª≠ d·ª•ng.

---

## ‚ú® T√≠nh nƒÉng

- ‚úÖ **Multi-provider support**: Naver, OpenAI, Cerebras, Gemini
- ‚úÖ **T√°ch ri√™ng input/output tokens**: Theo d√µi chi ti·∫øt token ƒë·∫ßu v√†o v√† ƒë·∫ßu ra
- ‚úÖ **Session tracking**: T·ªïng h·ª£p token usage qua nhi·ªÅu requests
- ‚úÖ **Decorator pattern**: T·ª± ƒë·ªông track tokens cho functions
- ‚úÖ **Context manager**: Qu·∫£n l√Ω session d·ªÖ d√†ng
- ‚úÖ **Export capabilities**: Xu·∫•t d·ªØ li·ªáu ra JSON
- ‚úÖ **Scalable design**: D·ªÖ th√™m provider m·ªõi

---

## üöÄ Quick Start

### 1. Import module

```python
from llm import (
    LLMFactory,
    TokenCounter,
    track_tokens,
    extract_token_usage
)
```

### 2. C√°ch s·ª≠ d·ª•ng c∆° b·∫£n

#### **C√°ch 1: Direct extraction (ƒë∆°n gi·∫£n nh·∫•t)**

```python
from llm import LLMFactory, extract_token_usage

# T·∫°o LLM
factory = LLMFactory()
llm = factory.create_llm(provider="openai")

# G·ªçi LLM
response = llm.invoke("What is artificial intelligence?")

# Extract token usage
usage = extract_token_usage(response, provider="openai")

print(f"Input tokens: {usage.input_tokens}")
print(f"Output tokens: {usage.output_tokens}")
print(f"Total tokens: {usage.total_tokens}")
```

#### **C√°ch 2: V·ªõi TokenCounter (tracking nhi·ªÅu requests)**

```python
from llm import LLMFactory, TokenCounter

factory = LLMFactory()
llm = factory.create_llm(provider="cerebras")

# Kh·ªüi t·∫°o counter
counter = TokenCounter(provider="cerebras")

# Th·ª±c hi·ªán nhi·ªÅu requests
prompts = [
    "What is AI?",
    "Explain machine learning",
    "What is deep learning?"
]

for prompt in prompts:
    response = llm.invoke(prompt)
    counter.track_response(response)  # T·ª± ƒë·ªông extract v√† add

# In summary
counter.print_summary()
```

Output:
```
======================================================================
üìä Token Usage Summary - CEREBRAS
======================================================================
Total Requests: 3
Total Tokens: 450
  ‚îî‚îÄ Input Tokens: 120
  ‚îî‚îÄ Output Tokens: 330
Average per Request: 150.0
======================================================================

Detailed Breakdown:

  Request #1:
    Model: llama3.1-8b
    Input: 35 | Output: 95 | Total: 130
    Timestamp: 2025-11-08T10:30:45.123456

  Request #2:
    Model: llama3.1-8b
    Input: 42 | Output: 118 | Total: 160
    Timestamp: 2025-11-08T10:30:47.234567

  Request #3:
    Model: llama3.1-8b
    Input: 43 | Output: 117 | Total: 160
    Timestamp: 2025-11-08T10:30:49.345678
```

#### **C√°ch 3: Context Manager (best for sessions)**

```python
from llm import LLMFactory, TokenCounter

factory = LLMFactory()
llm = factory.create_llm(provider="gemini")

# Session t·ª± ƒë·ªông print summary khi k·∫øt th√∫c
with TokenCounter.session(provider="gemini") as counter:
    # Multiple LLM calls
    response1 = llm.invoke("Explain quantum computing")
    counter.track_response(response1)
    
    response2 = llm.invoke("What are its applications?")
    counter.track_response(response2)
    
    # Summary t·ª± ƒë·ªông in ra khi exit context
```

#### **C√°ch 4: Decorator (auto-tracking)**

```python
from llm import LLMFactory, track_tokens

factory = LLMFactory()
llm = factory.create_llm(provider="openai")

@track_tokens(provider="openai", auto_print=True)
def ask_question(prompt: str):
    return llm.invoke(prompt)

# T·ª± ƒë·ªông print token usage
result = ask_question("What is the meaning of life?")
```

Output:
```
üí° Token Usage:
   Provider: openai
   Model: gpt-4o-mini
   Input: 45 tokens
   Output: 127 tokens
   Total: 172 tokens
```

---

## üìò Structured Output Support

### V·ªõi `include_raw=True`

```python
from pydantic import BaseModel, Field
from llm import LLMFactory, TokenCounter

# Define schema
class Weather(BaseModel):
    location: str = Field(description="ƒê·ªãa ƒëi·ªÉm")
    temperature: float = Field(description="Nhi·ªát ƒë·ªô ¬∞C")

# T·∫°o structured LLM
factory = LLMFactory()
llm = factory.create_llm(provider="cerebras")
structured_llm = llm.with_structured_output(Weather, include_raw=True)

# Invoke
result = structured_llm.invoke("H√† N·ªôi h√¥m nay 27 ƒë·ªô C")

# Extract token usage
counter = TokenCounter(provider="cerebras")
usage = counter.extract_usage(result["raw"])  # ‚úÖ Pass raw response

print(f"Parsed: {result['parsed']}")
print(f"Tokens: {usage.total_tokens}")
```

### V·ªõi method helper trong Factory

**Option 1: T√≠ch h·ª£p v√†o Factory (recommended)**

```python
# Th√™m v√†o llm_factory.py
def create_structured_llm_with_tracking(
    self,
    schema: type[BaseModel],
    provider: ProviderType = "naver",
    **kwargs
):
    """Create structured LLM with automatic token tracking"""
    provider_instance = self.create_provider(provider, **kwargs)
    base_llm = provider_instance.get_llm()
    
    # Always include raw for token tracking
    return base_llm.with_structured_output(schema, include_raw=True)
```

**Option 2: Wrapper function**

```python
from llm import LLMFactory, TokenCounter

def invoke_with_tracking(llm, prompt, provider="naver"):
    """Helper to invoke and track tokens"""
    response = llm.invoke(prompt)
    
    counter = TokenCounter(provider=provider)
    usage = counter.extract_usage(response)
    
    return {
        "response": response,
        "usage": usage
    }

# Usage
factory = LLMFactory()
llm = factory.create_structured_llm(Weather, provider="openai")
result = invoke_with_tracking(llm, "Weather in Tokyo", provider="openai")

print(result["response"])
print(result["usage"])
```

---

## üîß Advanced Features

### 1. Export to JSON

```python
from llm import TokenCounter

counter = TokenCounter(provider="naver")

# ... track multiple responses ...

# Export to file
counter.export_to_json("token_usage_report.json")
```

### 2. Get aggregated statistics

```python
summary = counter.get_summary()

print(f"Total requests: {summary['total_requests']}")
print(f"Total tokens: {summary['total_tokens']}")
print(f"Average per request: {summary['average_tokens_per_request']}")

# Access individual requests
for request in summary['requests']:
    print(request['input_tokens'], request['output_tokens'])
```

### 3. Reset counter

```python
counter.reset()  # Clear all tracked data
```

---

## üèóÔ∏è Architecture

### Class Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  TokenUsage     ‚îÇ  ‚Üê Data model (Pydantic-like dataclass)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üë
        ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ TokenExtractor  ‚îÇ  ‚Üê Provider-specific extraction logic
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üë
        ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  TokenCounter   ‚îÇ  ‚Üê Main API & aggregation
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üë
        ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Decorators &   ‚îÇ  ‚Üê Convenience functions
‚îÇ  Utilities      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Provider Extraction Mapping

| Provider  | Metadata Key       | Input Field         | Output Field            |
|-----------|--------------------|---------------------|-------------------------|
| Naver     | `token_usage`      | `prompt_tokens`     | `completion_tokens`     |
| OpenAI    | `token_usage`      | `prompt_tokens`     | `completion_tokens`     |
| Cerebras  | `token_usage` or `usage_metadata` | `prompt_tokens`/`input_tokens` | `completion_tokens`/`output_tokens` |
| Gemini    | `usage_metadata`   | `prompt_token_count` | `candidates_token_count` |

---

## ‚ûï Th√™m Provider M·ªõi

### B∆∞·ªõc 1: Th√™m extraction method v√†o `TokenExtractor`

```python
# Trong llm/token_counter.py

class TokenExtractor:
    # ... existing methods ...
    
    @staticmethod
    def extract_from_new_provider(response) -> TokenUsage:
        """Extract token usage from NewProvider response"""
        md = getattr(response, "response_metadata", {}) or {}
        usage = md.get("usage_info", {})  # Provider-specific key
        
        return TokenUsage(
            provider="new_provider",
            input_tokens=usage.get("input_count"),  # Provider-specific field
            output_tokens=usage.get("output_count"),
            total_tokens=usage.get("total_count"),
            model=md.get("model"),
            raw=usage
        )
```

### B∆∞·ªõc 2: ƒêƒÉng k√Ω extractor

```python
class TokenCounter:
    EXTRACTORS = {
        "naver": TokenExtractor.extract_from_naver,
        "openai": TokenExtractor.extract_from_openai,
        "cerebras": TokenExtractor.extract_from_cerebras,
        "gemini": TokenExtractor.extract_from_gemini,
        "new_provider": TokenExtractor.extract_from_new_provider,  # ‚úÖ Add here
    }
```

### B∆∞·ªõc 3: S·ª≠ d·ª•ng

```python
counter = TokenCounter(provider="new_provider")
usage = counter.extract_usage(response)
```

**Ch·ªâ c·∫ßn 2 b∆∞·ªõc - r·∫•t d·ªÖ m·ªü r·ªông! üöÄ**

---

## üß™ Testing

```python
# test/test_token_counter.py
from llm import LLMFactory, TokenCounter, extract_token_usage

def test_token_extraction():
    """Test token extraction across all providers"""
    factory = LLMFactory()
    providers = ["naver", "openai", "cerebras", "gemini"]
    
    for provider in providers:
        if not factory.PROVIDERS[provider].is_available():
            print(f"‚ö†Ô∏è Skipping {provider} - API key not set")
            continue
        
        print(f"\n‚úÖ Testing {provider}...")
        
        llm = factory.create_llm(provider=provider)
        response = llm.invoke("Say hello")
        
        usage = extract_token_usage(response, provider=provider)
        
        assert usage.provider == provider
        assert usage.input_tokens is not None
        assert usage.output_tokens is not None
        assert usage.total_tokens is not None
        
        print(f"   Input: {usage.input_tokens}")
        print(f"   Output: {usage.output_tokens}")
        print(f"   Total: {usage.total_tokens}")

if __name__ == "__main__":
    test_token_extraction()
```

---

## üìä Use Cases

### 1. Cost Tracking

```python
from llm import TokenCounter

# Track costs per provider
COSTS = {
    "openai": {"input": 0.00015, "output": 0.0006},  # per 1K tokens
    "cerebras": {"input": 0.0001, "output": 0.0004},
}

counter = TokenCounter(provider="openai")

# ... perform requests ...

summary = counter.get_summary()
input_cost = (summary['total_input_tokens'] / 1000) * COSTS["openai"]["input"]
output_cost = (summary['total_output_tokens'] / 1000) * COSTS["openai"]["output"]
total_cost = input_cost + output_cost

print(f"Total cost: ${total_cost:.4f}")
```

### 2. Performance Monitoring

```python
from llm import TokenCounter
import time

counter = TokenCounter(provider="cerebras")

start = time.time()
response = llm.invoke(long_prompt)
duration = time.time() - start

usage = counter.track_response(response)

tokens_per_second = usage.output_tokens / duration
print(f"Speed: {tokens_per_second:.1f} tokens/sec")
```

### 3. A/B Testing

```python
from llm import LLMFactory, TokenCounter

factory = LLMFactory()

# Test 2 providers
counters = {
    "openai": TokenCounter(provider="openai"),
    "cerebras": TokenCounter(provider="cerebras"),
}

for provider, counter in counters.items():
    llm = factory.create_llm(provider=provider)
    response = llm.invoke(test_prompt)
    counter.track_response(response)
    
# Compare
for provider, counter in counters.items():
    summary = counter.get_summary()
    print(f"{provider}: {summary['total_tokens']} tokens")
```

---

## üìù Best Practices

1. **Lu√¥n d√πng `include_raw=True` v·ªõi structured output**
   ```python
   llm.with_structured_output(Schema, include_raw=True)
   ```

2. **D√πng context manager cho sessions**
   ```python
   with TokenCounter.session(provider="openai") as counter:
       # Your code here
   ```

3. **Export data ƒë·ªãnh k·ª≥**
   ```python
   counter.export_to_json(f"usage_{datetime.now().isoformat()}.json")
   ```

4. **Reset counter khi c·∫ßn**
   ```python
   counter.reset()  # Clear data sau m·ªói session
   ```

5. **Check availability tr∆∞·ªõc khi test**
   ```python
   if not factory.PROVIDERS[provider].is_available():
       print(f"Skipping {provider}")
   ```

---

## üéØ Summary

| Feature | C√°ch s·ª≠ d·ª•ng |
|---------|-------------|
| Quick extraction | `extract_token_usage(response, provider="openai")` |
| Session tracking | `TokenCounter(provider="naver")` |
| Auto-tracking | `@track_tokens(provider="cerebras")` |
| Context manager | `with TokenCounter.session(...) as counter:` |
| Export | `counter.export_to_json("report.json")` |
| Th√™m provider | Add method v√†o `TokenExtractor` + register |

---

‚úÖ **Token counter ƒë√£ s·∫µn s√†ng s·ª≠ d·ª•ng v·ªõi thi·∫øt k·∫ø scalable v√† d·ªÖ maintain!**
